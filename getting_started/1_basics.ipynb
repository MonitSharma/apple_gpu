{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `mlx.core` and make an `array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "a = mx.array([1, 2, 3,4])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlx.core.int32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = mx.array([1.0,2.0,3.0,4.0])\n",
    "b.dtype\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations in MLX are lazy. The outputs of MLX are not computed until they are needed. To force an array to be evalated use `eval()`. Arrays will automatically be evaluated in few cases. For example, inspecting a scalar with `array.item()`, printing an array or converting an array from  `array.numpy.ndarray` all automatically evaluate the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 4, 6, 8], dtype=float32)\n",
      "array([2, 4, 6, 8], dtype=float32)\n",
      "array([2, 4, 6, 8], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = a+b  # c is not evaluated yet\n",
    "mx.eval(c)  # c is evaluated now\n",
    "print(c)\n",
    "# or \n",
    "\n",
    "c = a+b # c is not evaluated yet\n",
    "print(c) # c is evaluated now\n",
    "\n",
    "# or\n",
    "\n",
    "c = a+b # c is not evaluated yet\n",
    "import numpy as np\n",
    "np.array(c) # c is evaluated now\n",
    "print(c) # c is evaluated now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Lazy Evaluation?\n",
    "\n",
    "When you perform operations in MLX, no computation actually happens. Instead a compute graph is recorded. The actual computation only happens if an `eval()` is performed.\n",
    "\n",
    "MLX uses lazy evaluation because it has some nice features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Compute Graphhs\n",
    "\n",
    "- What is a Compute Graph?\n",
    "\n",
    "A compute graph is like a map of all the operations needed to perform a computation. It shows how inputs are processed step-by-step to produce outputs.\n",
    "\n",
    "- What is Lazy Evaluation?\n",
    "\n",
    "Lazy evaluation means that instead of performing calculations immediately, we \"record\" the operations in a compute graph and wait to execute them until they are actually needed.\n",
    "\n",
    "- Why is This Useful?\n",
    "Recording operations lets us do powerful things like:\n",
    "\n",
    "1. Transform Functions: Tools like grad() (for computing gradients) and vmap() (for vectorized computations) can use the graph to modify or optimize the original function.\n",
    "2. Optimize the Graph: The graph can be reorganized to make computations faster or more efficient before running them.\n",
    "\n",
    "- How is This Done in MLX?\n",
    "MLX does not pre-compile or store reusable compute graphs. Instead, it creates them dynamically as you run your program.\n",
    "Lazy evaluation makes it easier to add features like compilation in the future, which could improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Compute What You Use\n",
    "\n",
    "- What Does This Mean?\n",
    "\n",
    "In MLX, if an output from the compute graph is not needed, it won’t be computed. For example:\n",
    "If you create a function with multiple outputs but only use one of them, MLX will skip the unnecessary calculations.\n",
    "\n",
    "- Why Is This Helpful?\n",
    "\n",
    "It saves time and computational resources because you don’t have to manually control or optimize which outputs are computed. MLX handles this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use `eval()`?\n",
    "\n",
    "The key decision in using `eval()` is balancing the size of the compute graph and the overhead of graph evaluation. \n",
    "\n",
    "\n",
    "1. **Trade-offs in Using `eval()`**\n",
    "\n",
    "\n",
    "- If you call eval() after every small computation, you incur fixed overhead repeatedly.\n",
    "- Example: The provided \"bad idea\" code snippet does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    a = a + b\n",
    "    mx.eval(a)  # Frequent evaluations\n",
    "    b = b * 2\n",
    "    mx.eval(b)  # Frequent evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drawbacks:\n",
    "\n",
    "    - High overhead due to repeated graph evaluations.\n",
    "    - Inefficient use of resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Large Graphs with Delayed `eval()` Calls:**\n",
    "    - If you delay `eval()` too long, the compute graph can become excessively large.\n",
    "    - While large graphs are computationally correct, they can become costly due to increased memory usage and graph traversal overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `eval()` at a balance point where the compute graph is large enough to minimize frequent overhead, but not so large that it becomes inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function and Graph Transformations\n",
    "\n",
    "MLX has standard function transformations like `grad()` and `vmap()`. Transformations can be composed arbitrarily. For example `grad(vmap(grad(fn)))` is allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `grad(fn)` (Gradient):\n",
    "    - Computes the derivative of the function `fn` with respect to its input.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mx.array(0.0)\n",
    "mx.grad(mx.sin)(x)  # Computes the derivative of sin(x) at x = 0\n",
    "# Output: array(1, dtype=float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vmap(fn)` (Vectorized Map):\n",
    "\n",
    "    - Applies `fn` over a batch of inputs simultaneously, vectorizing the computation for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine transformations to achieve more complex behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-0, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.grad(mx.grad(mx.sin))(x)\n",
    "# This computes the second derivative of sin(x), which is -sin(x) at x = 0.\n",
    "# Output: array(-0, dtype=float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Gradient Transformations\n",
    "\n",
    "MLX offers advanced transformations for working with gradients, such as `vjp` (Vector-Jacobian Product) and `jvp` (Jacobian-Vector Product). These tools are powerful for sensitivity analysis and computational efficiency when working with derivatives in machine learning, physics simulations, and optimization tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. `vjp(fn)` - Vector-Jacobian Product**\n",
    "\n",
    "- **What It Does:**\n",
    "  - Computes the product of a vector with the Jacobian of a function $ f(x) $.\n",
    "  - If $ J $ is the Jacobian matrix of $ f(x) $ and $ v $ is the vector, it calculates $ v \\cdot J $.\n",
    "\n",
    "- **Use Case:**\n",
    "  - Efficient for propagating gradients backward in reverse-mode differentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of fn(x): [array(9, dtype=float32)]\n",
      "Jacobian-Vector Product (J * v): [array(12, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function\n",
    "def fn(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Input\n",
    "x = mx.array(3.0)\n",
    "v = mx.array(2.0)  # The vector we want to multiply with the Jacobian\n",
    "\n",
    "# Compute jvp\n",
    "output, jvp_result = mx.jvp(fn, (x,), (v,))\n",
    "\n",
    "print(f\"Output of fn(x): {output}\")  # Output: 9.0\n",
    "print(f\"Jacobian-Vector Product (J * v): {jvp_result}\")  # Output: 6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation:**\n",
    "  - The function $ f(x) = x^2 $ has a derivative (Jacobian) $ J = 2x $.\n",
    "  - For $ x = 3 $, $ J = 6 $, and $ v = 2 $, the `vjp` calculates $ v \\cdot J = 2 \\cdot 6 = 12 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. `jvp(fn)` - Jacobian-Vector Product**\n",
    "\n",
    "- **What It Does:**\n",
    "  - Computes the product of the Jacobian of a function $ f(x) $ with a vector $ v $.\n",
    "  - If $ J $ is the Jacobian, it calculates $ J \\cdot v $.\n",
    "\n",
    "- **Use Case:**\n",
    "  - Useful for forward-mode differentiation or analyzing how small changes in inputs affect outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of fn(x): [array(9, dtype=float32)]\n",
      "Jacobian-Vector Product (J * v): [array(6, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function\n",
    "def fn(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Input\n",
    "x = mx.array(3.0)\n",
    "v = mx.array(1.0)  # The vector we want to multiply with the Jacobian\n",
    "\n",
    "# Compute jvp\n",
    "output, jvp_result = mx.jvp(fn, (x,), (v,))\n",
    "\n",
    "print(f\"Output of fn(x): {output}\")  # Output: 9.0\n",
    "print(f\"Jacobian-Vector Product (J * v): {jvp_result}\")  # Output: 6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation:**\n",
    "  - The Jacobian of $ f(x) = x^2 $ is $ J = 2x $. \n",
    "  - For $ x = 3 $ and $ v = 1 $, the `jvp` computes $ J \\cdot v = 6 \\cdot 1 = 6 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. `value_and_grad(fn)`**\n",
    "\n",
    "- **What It Does:**\n",
    "  - Computes both the value of a function and its gradient in a single call.\n",
    "  - This is efficient because it avoids redundant computations.\n",
    "\n",
    "- **Use Case:**\n",
    "  - Ideal for machine learning tasks where both the loss value and its gradient are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the function: array(4, dtype=float32)\n",
      "Gradient of the function: array(4, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function\n",
    "def loss_fn(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "# Input\n",
    "x = mx.array(5.0)\n",
    "\n",
    "# Compute value and gradient\n",
    "value, grad = mx.value_and_grad(loss_fn)(x)\n",
    "\n",
    "print(f\"Value of the function: {value}\")  # Output: 4.0\n",
    "print(f\"Gradient of the function: {grad}\")  # Output: 4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation:**\n",
    "  - The loss function $f(x) = (x - 3)^2 $ has:\n",
    "    - Value $f(5) = (5 - 3)^2 = 4 $.\n",
    "    - Gradient $f'(x) = 2(x - 3) $, so $f'(5) = 2 \\cdot (5 - 3) = 4 $.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **`vjp(fn)`**: Use for propagating gradients backward efficiently.\n",
    "2. **`jvp(fn)`**: Use for forward-mode sensitivity analysis.\n",
    "3. **`value_and_grad(fn)`**: Use to compute both the output and gradient of a function in one step, saving time and resources.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
