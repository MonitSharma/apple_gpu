{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Memory\n",
    "\n",
    "Apple silicon has a unified memory architecture. The CPU and GPU have direct access to the same memory pool. MLX is designed to take advantage of that.\n",
    "\n",
    "Concretely, when you make an array in MLX, you don't have to specify its location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "a = mx.random.normal((10000000,))\n",
    "b = mx.random.normal((10000000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `a` and `b` live in unified memory.\n",
    "\n",
    "In MLX, rather than moving arrays to devices, you specify the device when you run the operation. Any device can perform any operation on `a` and `b` without needing to move them from one memory location ot another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 8.869171142578125e-05\n",
      "GPU time: 4.1961669921875e-05\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "mx.add(a,b,stream=mx.cpu)\n",
    "toc = time.time()\n",
    "print('CPU time:',toc-tic)\n",
    "\n",
    "tic = time.time()\n",
    "mx.add(a,b,stream=mx.gpu)\n",
    "toc = time.time()\n",
    "print('GPU time:',toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, both the CPU and the GPU will perform the same add operation. The operation can ( and likely) be run in parallel, since there are no dependencies between them.\n",
    "\n",
    "In the above `add` example, there are no dependencies between the operations, so there is no possibility for race conditions. If there are dependencies, the MLX scehduler will autmatically manage them. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.00011491775512695312\n",
      "GPU time: 4.38690185546875e-05\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = mx.add(a,b,stream=mx.cpu)\n",
    "toc = time.time()\n",
    "print('CPU time:',toc-tic)\n",
    "\n",
    "tic = time.time()\n",
    "c = mx.add(a,b,stream=mx.gpu)\n",
    "toc = time.time()\n",
    "print('GPU time:',toc-tic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, the second `add` runs on the GPU but it depends on the output of the first `add` which is running on CPU. MLX will automatically insert a dependency between two streasm so that the second `add` only starts executing after the first is complete and `c` is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "Here is a more interesting example of how unified memory can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a,b,d1,d2):\n",
    "    x = mx.matmul(a,b,stream=d1)\n",
    "    for _ in range(500):\n",
    "        b = mx.exp(b,stream=d2)\n",
    "    return x,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we want to run with the following arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mx.random.uniform(shape=(1000000000,10000000))\n",
    "b = mx.random.uniform(shape=(10000000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `matmul` operation is a good fit for GPY, since it's more compute dense. The second sequence of operatins are better fit for the CPU, since they are very small and would probably be overhead bound on the GPU. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full GPU time: 0.0025260448455810547\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "fun(a,b,mx.gpu,mx.gpu)\n",
    "toc = time.time()\n",
    "print('Full GPU time:',toc-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full CPU time: 0.0014491081237792969\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "fun(a,b,mx.cpu,mx.cpu)\n",
    "toc = time.time()\n",
    "print('Full CPU time:',toc-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU + CPU time: 0.0014119148254394531\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "fun(a,b,mx.gpu,mx.cpu)\n",
    "toc = time.time()\n",
    "print('GPU + CPU time:',toc-tic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
