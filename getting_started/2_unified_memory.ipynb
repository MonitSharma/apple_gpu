{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Memory in MLX\n",
    "\n",
    "Apple Silicon features a **unified memory architecture**, allowing the CPU and GPU to share the same memory pool. MLX leverages this architecture to simplify and optimize memory usage for computations.\n",
    "\n",
    "\n",
    "\n",
    "### **1. What is Unified Memory?**\n",
    "\n",
    "- In traditional architectures, the CPU and GPU use separate memory pools, requiring explicit data transfers between them.\n",
    "- With unified memory, the CPU and GPU can directly access the same memory pool, eliminating the overhead of data transfers.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Array Location in MLX**\n",
    "\n",
    "- When you create an array in MLX, its location (CPU or GPU) does not need to be specified.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "a = mx.random.normal((100,))\n",
    "b = mx.random.normal((100,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Both `a` and `b` are stored in unified memory and accessible by both the CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Device-Specific Operations**\n",
    "\n",
    "- Instead of moving arrays between devices, you specify the device when performing an operation.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.586388, 0.780082, 0.683154, ..., 2.48238, 0.565931, -1.35311], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.add(a, b, stream=mx.cpu)  # Perform addition on the CPU\n",
    "mx.add(a, b, stream=mx.gpu)  # Perform addition on the GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Both operations directly access `a` and `b` in unified memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a: array([-0.035678, -1.62694, -0.229797, ..., 0.374411, -1.10128, -0.0897107], dtype=float32)\n",
      "Array b: array([0.0952313, -0.0990817, 0.301924, ..., 0.0235381, 0.815937, -0.1246], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create arrays in unified memory\n",
    "a = mx.random.normal((100,))\n",
    "b = mx.random.normal((100,))\n",
    "\n",
    "# Check the arrays\n",
    "print(f\"Array a: {a}\")\n",
    "print(f\"Array b: {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Parallelism and Streams**\n",
    "\n",
    "- Since the CPU and GPU can share memory, they can perform operations in parallel.\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on CPU: array([0.0595533, -1.72602, 0.072127, ..., 0.39795, -0.285342, -0.214311], dtype=float32)\n",
      "Result on GPU: array([0.0595533, -1.72602, 0.072127, ..., 0.39795, -0.285342, -0.214311], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Perform addition on the CPU\n",
    "result_cpu = mx.add(a, b, stream=mx.cpu)\n",
    "print(f\"Result on CPU: {result_cpu}\")\n",
    "\n",
    "# Perform addition on the GPU\n",
    "result_gpu = mx.add(a, b, stream=mx.gpu)\n",
    "print(f\"Result on GPU: {result_gpu}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - These operations can run simultaneously since there are no dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0595533, -1.72602, 0.072127, ..., 0.39795, -0.285342, -0.214311], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform parallel operations\n",
    "mx.add(a, b, stream=mx.cpu)\n",
    "mx.add(a, b, stream=mx.gpu)\n",
    "# Both operations access the same unified memory and can execute in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Handling Dependencies**\n",
    "\n",
    "- If operations have dependencies, MLX's scheduler automatically ensures they are handled correctly, preventing race conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLX will automatically insert a dependency between the two streams so that the second add only starts executing after the first is complete and `a` and `b` are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: Using Unified Memory for Optimal Device Utilization\n",
    "\n",
    "Unified memory in Apple Silicon allows seamless sharing of data between the CPU and GPU, enabling mixed execution to optimize performance. Here's an example illustrating this:\n",
    "\n",
    "\n",
    "\n",
    "### **Computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a computation that involves:\n",
    "1. A **matrix multiplication** (`mx.matmul`) between large matrices `a` and `b`.\n",
    "2. **Repeated element-wise operations** (`mx.exp`) on matrix `b`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Device Suitability**\n",
    "\n",
    "1. **Matrix Multiplication (`mx.matmul`):**\n",
    "   - This operation is compute-intensive and benefits significantly from the parallel processing power of the **GPU**.\n",
    "\n",
    "2. **Repeated Element-Wise Operations (`mx.exp`):**\n",
    "   - These operations are small and frequent, meaning:\n",
    "     - Running them on the GPU could introduce overhead due to kernel launch times.\n",
    "     - The **CPU** is better suited for these smaller tasks because it has lower overhead for such operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Unified Memory Advantage**\n",
    "\n",
    "- Both `a` and `b` reside in **unified memory**, accessible by both the **CPU** and **GPU**.\n",
    "- This allows seamless sharing of data between devices **without explicit data transfers**.\n",
    "- Unified memory saves time and simplifies the code by removing the need to manually move data between devices.\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance Comparison**\n",
    "\n",
    "#### **Fully on GPU:**\n",
    "- Running all operations on the GPU takes approximately **2.8 milliseconds**.\n",
    "- The small, frequent operations (`mx.exp`) are **overhead-bound**, making the GPU less efficient for this part of the computation.\n",
    "\n",
    "#### **Mixed Execution (GPU + CPU):**\n",
    "- By splitting the workload:\n",
    "  - The **GPU** handles the compute-intensive matrix multiplication.\n",
    "  - The **CPU** efficiently processes the small, frequent element-wise operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: arm\n",
      "\n",
      "Running fully on GPU...\n",
      "Full GPU Execution: 0.0017 seconds\n",
      "\n",
      "Running mixed execution (GPU for matmul, CPU for exp)...\n",
      "Mixed GPU + CPU Execution: 0.0016 seconds\n",
      "\n",
      "--- Performance Summary ---\n",
      "Chip: arm\n",
      "Full GPU Execution Time: 0.0017 seconds\n",
      "Mixed Execution Time: 0.0016 seconds\n",
      "Speedup (Mixed vs GPU-only): 1.10x faster\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import platform\n",
    "\n",
    "# Define the computation function\n",
    "def fun(a, b, d1, d2):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication on device d1 and repeated element-wise\n",
    "    operations on device d2.\n",
    "    \"\"\"\n",
    "    # Perform matrix multiplication on device d1\n",
    "    x = mx.matmul(a, b, stream=d1)\n",
    "    # Perform repeated element-wise operations on device d2\n",
    "    for _ in range(500):\n",
    "        b = mx.exp(b, stream=d2)\n",
    "    return x, b\n",
    "\n",
    "# Create large arrays in unified memory\n",
    "a = mx.random.uniform(shape=(4096, 5120))  # Large matrix\n",
    "b = mx.random.uniform(shape=(5120, 400))    # Smaller matrix\n",
    "\n",
    "# Function to time and execute\n",
    "def execute_and_time(func, *args, description=\"Execution\"):\n",
    "    start_time = time.time()\n",
    "    result = func(*args)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{description}: {elapsed_time:.4f} seconds\")\n",
    "    return result, elapsed_time\n",
    "\n",
    "# Display the system information\n",
    "chip_name = platform.processor()\n",
    "print(f\"Running on: {chip_name}\")\n",
    "\n",
    "# Fully on GPU\n",
    "print(\"\\nRunning fully on GPU...\")\n",
    "(x_gpu, b_gpu), time_gpu = execute_and_time(fun, a, b, mx.gpu, mx.gpu, description=\"Full GPU Execution\")\n",
    "\n",
    "# Mixed execution (GPU + CPU)\n",
    "print(\"\\nRunning mixed execution (GPU for matmul, CPU for exp)...\")\n",
    "(x_mixed, b_mixed), time_mixed = execute_and_time(fun, a, b, mx.gpu, mx.cpu, description=\"Mixed GPU + CPU Execution\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n--- Performance Summary ---\")\n",
    "print(f\"Chip: {chip_name}\")\n",
    "print(f\"Full GPU Execution Time: {time_gpu:.4f} seconds\")\n",
    "print(f\"Mixed Execution Time: {time_mixed:.4f} seconds\")\n",
    "print(f\"Speedup (Mixed vs GPU-only): {time_gpu / time_mixed:.2f}x faster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
